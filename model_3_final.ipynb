{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Model**\n",
    " **[Target, Result, Analysis] at the end**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase transformations\n",
    "train_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       # transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
    "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values.\n",
    "                                       # Note the difference between (0.1307) and (0.1307,)\n",
    "                                       ])\n",
    "\n",
    "# Test Phase transformations\n",
    "test_transforms = transforms.Compose([\n",
    "                                      #  transforms.Resize((28, 28)),\n",
    "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
    "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? mps\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'function' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m dataloader_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# train dataloader\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataloader_args)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# test dataloader\u001b[39;00m\n\u001b[1;32m     23\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataloader_args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py:163\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py:172\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_source)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'function' has no len()"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "# CUDA?\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"GPU Available?\", device)\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "elif device == \"mps\":\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "# dataloader arguments - something you'll fetch these from cmdprmt\n",
    "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if device else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
    "\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n",
    "\n",
    "# Pretty table for collecting all the accuracy and loss parameters in a table\n",
    "log_table = PrettyTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_value = 0.01\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Input Block\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 26\n",
    "\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 24\n",
    "\n",
    "         # TRANSITION BLOCK 1\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            nn.ReLU()\n",
    "        ) # output_size = 12\n",
    "\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 10\n",
    "\n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 8\n",
    "\n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=12, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 6\n",
    "\n",
    "        # OUTPUT BLOCK\n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_value)\n",
    "        ) # output_size = 6\n",
    "\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=6) # 7>> 9... nn.AdaptiveAvgPool((1, 1))\n",
    "        ) # output_size = 1\n",
    "\n",
    "        self.convblock8 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
    "            # nn.BatchNorm2d(10), NEVER\n",
    "            # nn.ReLU() NEVER!\n",
    "        ) # output_size = 1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblock3(x) #transition\n",
    "\n",
    "        x = self.convblock4(x)\n",
    "        x = self.convblock5(x)\n",
    "        x = self.convblock6(x)\n",
    "        x = self.convblock7(x) #padding=1\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = self.convblock8(x) #transition\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 26, 26]              72\n",
      "              ReLU-2            [-1, 8, 26, 26]               0\n",
      "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
      "           Dropout-4            [-1, 8, 26, 26]               0\n",
      "            Conv2d-5           [-1, 16, 24, 24]           1,152\n",
      "              ReLU-6           [-1, 16, 24, 24]               0\n",
      "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
      "           Dropout-8           [-1, 16, 24, 24]               0\n",
      "         MaxPool2d-9           [-1, 16, 12, 12]               0\n",
      "           Conv2d-10            [-1, 8, 12, 12]             128\n",
      "             ReLU-11            [-1, 8, 12, 12]               0\n",
      "           Conv2d-12           [-1, 12, 10, 10]             864\n",
      "             ReLU-13           [-1, 12, 10, 10]               0\n",
      "      BatchNorm2d-14           [-1, 12, 10, 10]              24\n",
      "          Dropout-15           [-1, 12, 10, 10]               0\n",
      "           Conv2d-16             [-1, 12, 8, 8]           1,296\n",
      "             ReLU-17             [-1, 12, 8, 8]               0\n",
      "      BatchNorm2d-18             [-1, 12, 8, 8]              24\n",
      "          Dropout-19             [-1, 12, 8, 8]               0\n",
      "           Conv2d-20             [-1, 16, 6, 6]           1,728\n",
      "             ReLU-21             [-1, 16, 6, 6]               0\n",
      "      BatchNorm2d-22             [-1, 16, 6, 6]              32\n",
      "          Dropout-23             [-1, 16, 6, 6]               0\n",
      "           Conv2d-24             [-1, 16, 6, 6]           2,304\n",
      "             ReLU-25             [-1, 16, 6, 6]               0\n",
      "      BatchNorm2d-26             [-1, 16, 6, 6]              32\n",
      "          Dropout-27             [-1, 16, 6, 6]               0\n",
      "        AvgPool2d-28             [-1, 16, 1, 1]               0\n",
      "           Conv2d-29             [-1, 10, 1, 1]             160\n",
      "================================================================\n",
      "Total params: 7,864\n",
      "Trainable params: 7,864\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.58\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "cuda = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(cuda)\n",
    "model = Net().to(cuda)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "  pbar = tqdm(train_loader)\n",
    "  correct = 0\n",
    "  processed = 0\n",
    "  for batch_idx, (data, target) in enumerate(pbar):\n",
    "    # get samples\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Init\n",
    "    optimizer.zero_grad()\n",
    "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
    "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.nll_loss(y_pred, target)\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update pbar-tqdm\n",
    "\n",
    "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    processed += len(data)\n",
    "\n",
    "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "    train_acc.append(100*correct/processed)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model running on:  mps\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.028291886672377586 Batch_id=468 Accuracy=94.47: 100%|██████████| 469/469 [00:10<00:00, 43.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0770, Accuracy: 9762/10000 (97.62%)\n",
      "\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.01176320482045412 Batch_id=468 Accuracy=98.11: 100%|██████████| 469/469 [00:09<00:00, 48.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0396, Accuracy: 9869/10000 (98.69%)\n",
      "\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0807792991399765 Batch_id=468 Accuracy=98.39: 100%|██████████| 469/469 [00:09<00:00, 48.56it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0460, Accuracy: 9863/10000 (98.63%)\n",
      "\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.009636742062866688 Batch_id=468 Accuracy=98.63: 100%|██████████| 469/469 [00:09<00:00, 48.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0250, Accuracy: 9910/10000 (99.10%)\n",
      "\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.009648158214986324 Batch_id=468 Accuracy=98.75: 100%|██████████| 469/469 [00:09<00:00, 48.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9863/10000 (98.63%)\n",
      "\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.01151130348443985 Batch_id=468 Accuracy=98.84: 100%|██████████| 469/469 [00:09<00:00, 47.87it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9917/10000 (99.17%)\n",
      "\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0029709378723055124 Batch_id=468 Accuracy=99.16: 100%|██████████| 469/469 [00:09<00:00, 47.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0187, Accuracy: 9941/10000 (99.41%)\n",
      "\n",
      "EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0022046410012990236 Batch_id=468 Accuracy=99.36: 100%|██████████| 469/469 [00:09<00:00, 48.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0181, Accuracy: 9943/10000 (99.43%)\n",
      "\n",
      "EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.01676405407488346 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:09<00:00, 48.10it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0168, Accuracy: 9948/10000 (99.48%)\n",
      "\n",
      "EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.01024116575717926 Batch_id=468 Accuracy=99.44: 100%|██████████| 469/469 [00:09<00:00, 47.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0187, Accuracy: 9941/10000 (99.41%)\n",
      "\n",
      "EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.002983597805723548 Batch_id=468 Accuracy=99.40: 100%|██████████| 469/469 [00:09<00:00, 47.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0178, Accuracy: 9946/10000 (99.46%)\n",
      "\n",
      "EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.007639870047569275 Batch_id=468 Accuracy=99.46: 100%|██████████| 469/469 [00:09<00:00, 47.63it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0173, Accuracy: 9943/10000 (99.43%)\n",
      "\n",
      "EPOCH: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0024651766289025545 Batch_id=468 Accuracy=99.48: 100%|██████████| 469/469 [00:09<00:00, 47.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0175, Accuracy: 9943/10000 (99.43%)\n",
      "\n",
      "EPOCH: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.003154776059091091 Batch_id=468 Accuracy=99.51: 100%|██████████| 469/469 [00:09<00:00, 47.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0172, Accuracy: 9944/10000 (99.44%)\n",
      "\n",
      "EPOCH: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=0.0023794518783688545 Batch_id=468 Accuracy=99.55: 100%|██████████| 469/469 [00:09<00:00, 47.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0170, Accuracy: 9945/10000 (99.45%)\n",
      "\n",
      "+-------+-------------------+---------------+-------+---------------+-----------+\n",
      "| Epoch | Training Accuracy | Test Accuracy |  Diff | Training Loss | Test Loss |\n",
      "+-------+-------------------+---------------+-------+---------------+-----------+\n",
      "|   0   |       94.47%      |     97.62%    | -3.15 |     0.0283    |   0.0770  |\n",
      "|   1   |       98.11%      |     98.69%    | -0.58 |     0.0118    |   0.0396  |\n",
      "|   2   |       98.39%      |     98.63%    | -0.24 |     0.0808    |   0.0460  |\n",
      "|   3   |       98.63%      |     99.10%    | -0.47 |     0.0096    |   0.0250  |\n",
      "|   4   |       98.75%      |     98.63%    |  0.12 |     0.0096    |   0.0398  |\n",
      "|   5   |       98.84%      |     99.17%    | -0.33 |     0.0115    |   0.0267  |\n",
      "|   6   |       99.16%      |     99.41%    | -0.25 |     0.0030    |   0.0187  |\n",
      "|   7   |       99.36%      |     99.43%    | -0.07 |     0.0022    |   0.0181  |\n",
      "|   8   |       99.42%      |     99.48%    | -0.06 |     0.0168    |   0.0168  |\n",
      "|   9   |       99.44%      |     99.41%    |  0.03 |     0.0102    |   0.0187  |\n",
      "|   10  |       99.40%      |     99.46%    | -0.06 |     0.0030    |   0.0178  |\n",
      "|   11  |       99.46%      |     99.43%    |  0.03 |     0.0076    |   0.0173  |\n",
      "|   12  |       99.48%      |     99.43%    |  0.05 |     0.0025    |   0.0175  |\n",
      "|   13  |       99.51%      |     99.44%    |  0.07 |     0.0032    |   0.0172  |\n",
      "|   14  |       99.55%      |     99.45%    |  0.09 |     0.0024    |   0.0170  |\n",
      "+-------+-------------------+---------------+-------+---------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "print(\"model running on: \", device)\n",
    "log_table = PrettyTable()\n",
    "log_table.field_names = [\"Epoch\", \"Training Accuracy\", \"Test Accuracy\", \"Diff\", \"Training Loss\", \"Test Loss\"]\n",
    "\n",
    "model =  Net().to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
    "\n",
    "EPOCHS = 15\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    test(model, device, test_loader)\n",
    "    log_table.add_row([epoch, f\"{train_acc[-1]:.2f}%\", f\"{test_acc[-1]:.2f}%\", f\"{float(train_acc[-1]) - float(test_acc[-1]):.2f}\" ,f\"{train_losses[-1]:.4f}\", f\"{test_losses[-1]:.4f}\"])\n",
    "print(log_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TARGET\n",
    "- Here we introduced StepLR learning rate scheduler. \n",
    "\n",
    "- Experimented with optimiser as well.\n",
    "\n",
    "- Able to get desired result **consistently** from 7th epoch onwards, all documented as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULT\n",
    "\n",
    "- With **StepLR and SGD**\n",
    "\n",
    "    - Parameters            : **7864**\n",
    "\n",
    "    - Best Train Accuracy   : **99.30%**\n",
    "\n",
    "    - Best Test Accuracy    : **99.40%**\n",
    "---\n",
    "- After adding **StepLR and ADAM**\n",
    "\n",
    "    - Parameters            : **7864**\n",
    "\n",
    "    - Best Train Accuracy   : **99.55%**\n",
    "\n",
    "    - Best Test Accuracy    : **99.48%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "- **Model Architecture**\n",
    "   \n",
    "    - **Channels** :  1 → 8 → 16 → MaxPool → 16 → (Transition) → 8 → 12 → 12 → 16 → 16 → GAP → 16 → Transition → 10 \n",
    "\n",
    "    - **Receptive Field**\n",
    "\n",
    "        | Block               | Layer      | Input Size | Kernel x Stride | Padding | Input Channel | Output Channel | Output Size | Receptive Field |\n",
    "        | ------------------- | ---------- | ---------- | --------------- | ------- | ------------- | -------------- | ----------- | --------------- |\n",
    "        | Input Block         | Conv1      | 28         | 3 x 1           | 0       | 1             | 8              | 26          | 3               |\n",
    "        | Convolution Block 1 | Conv2      | 26         | 3 x 1           | 0       | 8             | 16             | 24          | 5               |\n",
    "        | Transition Block 1  | Pool1      | 24         | 2 x 2           | 0       | 16            | 16             | 12          | 6               |\n",
    "        | Transition Block 1  | Conv3      | 12         | 1 x 1           | 0       | 16            | 8              | 12          | 6               |\n",
    "        | Convolution Block 2 | Conv4      | 12         | 3 x 1           | 0       | 8             | 12             | 10          | 10              |\n",
    "        | Convolution Block 2 | conv5      | 10         | 3 x 1           | 0       | 12            | 12             | 8           | 14              |\n",
    "        | Convolution Block 2 | Conv6      | 8          | 3 x 1           | 0       | 12            | 16             | 6           | 18              |\n",
    "        | Convolution Block 2 | Conv7      | 6          | 3 x 1           | 1       | 16            | 16             | 6           | 22              |\n",
    "        | Output Block        | GAP        | 6          | 3 x 1           | 0       | 16            | 16             | 1           | 28              |\n",
    "        | Output Block        | Conv8      | 1          | 1 x 1           | 0       | 16            | 10             | 1           | 28              |\n",
    "\n",
    "- **Observation**\n",
    "    - After adding scheduler the model was able to achive its objective performance of 99.4% test accuracy under 15 epcoh under 800 parameters\n",
    "\n",
    "    - Here I am giving 2 results from 2 optimisers\n",
    "\n",
    "    ---\n",
    "\n",
    "    - **Leraning Rate scheduler with SGD**\n",
    "\n",
    "        \n",
    "        | Epoch | Training Accuracy | Test Accuracy |  Diff | Training Loss | Test Loss |\n",
    "        |-------|-------------------|---------------|-------|---------------|-----------|\n",
    "        |   0   |       88.16%      |     97.17%    | -9.01 |     0.1259    |   0.1030  |\n",
    "        |   1   |       97.83%      |     98.61%    | -0.78 |     0.0668    |   0.0510  |\n",
    "        |   2   |       98.31%      |     98.51%    | -0.20 |     0.0186    |   0.0499  |\n",
    "        |   3   |       98.45%      |     98.98%    | -0.53 |     0.0682    |   0.0353  |\n",
    "        |   4   |       98.67%      |     98.97%    | -0.30 |     0.0135    |   0.0356  |\n",
    "        |   5   |       98.81%      |     99.15%    | -0.34 |     0.0340    |   0.0280  |\n",
    "        |   6   |       99.08%      |     99.31%    | -0.23 |     0.0057    |   0.0228  |\n",
    "        |   7   |       99.17%      |     99.35%    | -0.18 |     0.0274    |   0.0219  |\n",
    "        |   8   |       99.21%      |     99.29%    | -0.08 |     0.1171    |   0.0220  |\n",
    "        |   9   |       99.25%      |     99.39%    | -0.14 |     0.0839    |   0.0214  |\n",
    "        |   10  |       99.20%      |     99.34%    | -0.14 |     0.0466    |   0.0223  |\n",
    "        |   11  |       99.23%      |     99.36%    | -0.13 |     0.0114    |   0.0212  |\n",
    "        |   12  |       99.24%      |     99.37%    | -0.13 |     0.0081    |   0.0216  |\n",
    "        |   13  |       99.25%      |     99.40%    | -0.15 |     0.0130    |   0.0211  |\n",
    "        |   14  |       99.30%      |     99.40%    | -0.10 |     0.0250    |   0.0208  |\n",
    "\n",
    "    ---\n",
    "\n",
    "    - **Learning Rate scheduler with ADAM optimiser**\n",
    "\n",
    "\n",
    "        | Epoch | Training Accuracy | Test Accuracy |  Diff | Training Loss | Test Loss |\n",
    "        |-------|-------------------|---------------|-------|---------------|-----------|\n",
    "        |   0   |       94.47%      |     97.62%    | -3.15 |     0.0283    |   0.0770  |\n",
    "        |   1   |       98.11%      |     98.69%    | -0.58 |     0.0118    |   0.0396  |\n",
    "        |   2   |       98.39%      |     98.63%    | -0.24 |     0.0808    |   0.0460  |\n",
    "        |   3   |       98.63%      |     99.10%    | -0.47 |     0.0096    |   0.0250  |\n",
    "        |   4   |       98.75%      |     98.63%    |  0.12 |     0.0096    |   0.0398  |\n",
    "        |   5   |       98.84%      |     99.17%    | -0.33 |     0.0115    |   0.0267  |\n",
    "        |   6   |       99.16%      |     99.41%    | -0.25 |     0.0030    |   0.0187  |\n",
    "        |   7   |       99.36%      |     99.43%    | -0.07 |     0.0022    |   0.0181  |\n",
    "        |   8   |       99.42%      |     99.48%    | -0.06 |     0.0168    |   0.0168  |\n",
    "        |   9   |       99.44%      |     99.41%    |  0.03 |     0.0102    |   0.0187  |\n",
    "        |   10  |       99.40%      |     99.46%    | -0.06 |     0.0030    |   0.0178  |\n",
    "        |   11  |       99.46%      |     99.43%    |  0.03 |     0.0076    |   0.0173  |\n",
    "        |   12  |       99.48%      |     99.43%    |  0.05 |     0.0025    |   0.0175  |\n",
    "        |   13  |       99.51%      |     99.44%    |  0.07 |     0.0032    |   0.0172  |\n",
    "        |   14  |       99.55%      |     99.45%    |  0.09 |     0.0024    |   0.0170  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
